name: Daily Supermarket Scraper Run

on:
  push:
    branches:
      - main
  # Descomenta la siguiente sección si deseas ejecutar este workflow en un horario programado
  # schedule:
  #   # Ejecuta todos los días a las 12:00 UTC
  #   - cron: '0 12 * * *'

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    env:
      GCS_PASSWORD: ${{ secrets.GCS_PASSWORD }}
      GCS_USER_ROOT: ${{ secrets.GCS_USER_ROOT }}
      GCS_DATABASE: ${{ secrets.GCS_DATABASE }}
      GCS_HOST: ${{ secrets.GCS_HOST }}
      GCP_CREDENTIALS: ${{ secrets.GCP_CREDENTIALS }} # Asegúrate de haber agregado esto a tus secrets en GitHub
      INSTANCE_CONNECTION_NAME: ${{ secrets.INSTANCE_CONNECTION_NAME }} # Añade esto también a tus secrets

    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'  # Asegúrate de usar la versión de Python que necesites

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v0
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        service_account_key: ${{ secrets.GCP_CREDENTIALS }}
        export_default_credentials: true

    - name: Install and start Cloud SQL Proxy
      run: |
        wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 -O cloud_sql_proxy
        chmod +x cloud_sql_proxy
        ./cloud_sql_proxy --address 0.0.0.0 --port 3306 ${{ secrets.INSTANCE_CONNECTION_NAME }} &
        sleep 10 # Espera a que el proxy inicie completamente

    - name: Install Chrome WebDriver
      run: |
        sudo apt-get update
        sudo apt-get install -y chromium-driver
        echo "CHROMEWEBDRIVER=/usr/bin/chromium-driver" >> $GITHUB_ENV

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install selenium webdriver_manager

    - name: Get Public IP
      run: curl ifconfig.me

    - name: Run script
      run: python main.py --correr-todo

